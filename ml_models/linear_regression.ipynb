{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ded520b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# if you ever use GPU, these help determinism\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c483359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts 'MM:SS' strings to float minutes\n",
    "def convert_mp(mp):\n",
    "    if isinstance(mp, str):\n",
    "        if ':' in mp:\n",
    "            try:\n",
    "                mins, secs = mp.split(':')\n",
    "                return int(mins) + int(secs) / 60\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                return float(mp)\n",
    "            except:\n",
    "                return None\n",
    "    elif isinstance(mp, (int, float)):\n",
    "        return float(mp)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3c02b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "#data = pd.read_csv('../data-collection/bbref_players_games_simple/g/gilgesh01_Shai_Gilgeous-Alexander_last3.csv')\n",
    "data = pd.read_csv('../data-collection/bbref_players_games_simple/gilgesh01_Shai_Gilgeous-Alexander_last3_with_opp_stats.csv')\n",
    "# make sure games are ordered oldest -> newest\n",
    "if 'Date' in data.columns:\n",
    "    data = data.sort_values('Date')\n",
    "\n",
    "# is_home: 1 if home, 0 if away\n",
    "data['is_home'] = (data['Unnamed: 5'] != '@').astype(float)\n",
    "# result_win: 1 if team won, 0 if lost\n",
    "data['result_win'] = data['Result'].str.startswith('W').astype(float)\n",
    "\n",
    "# features (previous game)\n",
    "features = [\n",
    "    'MP', 'FGA', '3PA', 'FTA',\n",
    "    'FG%', '3P%', '2P%', 'eFG%', 'FT%',\n",
    "    'TRB', 'AST', 'TOV',\n",
    "    'GmSc', '+/-', 'PTS', 'is_home', 'result_win',\n",
    "    'OppOffRtg', 'OppDefRtg', 'OppNetRtg', 'OppPace'\n",
    "]\n",
    "\n",
    "# convert MP to float minutes\n",
    "data['MP'] = data['MP'].apply(convert_mp)\n",
    "\n",
    "# convert numeric columns\n",
    "for col in features + ['PTS']:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# create TARGET from *current* game points (before shifting)\n",
    "current_ppg = 32.4\n",
    "data['above_ppg'] = (data['PTS'] > current_ppg).astype(float)\n",
    "target = ['above_ppg']\n",
    "\n",
    "# now shift features so they refer to the *previous* game\n",
    "data[features] = data[features].shift(1)\n",
    "\n",
    "# drop first row and rows with NaNs\n",
    "data = data.dropna(subset=features + target)\n",
    "\n",
    "# numpy arrays\n",
    "X_np = data[features].values\n",
    "y_np = data[target].values\n",
    "\n",
    "# chronological split: last 20 games as test\n",
    "n_test = 20\n",
    "X_train_np, X_test_np = X_np[:-n_test], X_np[-n_test:]\n",
    "y_train_np, y_test_np = y_np[:-n_test], y_np[-n_test:]\n",
    "\n",
    "# to torch\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "aeeb3e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[-0.0095, -0.0629, -0.0513, -0.0322,  0.1520, -0.3797, -0.3943,  0.4327,\n",
      "          0.6446,  0.0078,  0.0310,  0.0357, -0.0345, -0.0046,  0.0712, -0.1047,\n",
      "         -0.0242,  0.0694, -0.0578, -0.0574, -0.0148]])\n",
      "Bias: tensor([-0.0363])\n"
     ]
    }
   ],
   "source": [
    "# define model, loss function, and optimizer\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "epochs = 1000\n",
    "lambda_penalty = .8  # tune this (start small like 0.1â€“0.5)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    probs = model(X_train)\n",
    "\n",
    "    # base BCE loss\n",
    "    bce = criterion(probs, y_train)\n",
    "\n",
    "    # confidence in [0,1]: 0 at 0.5, 1 near 0 or 1\n",
    "    confidence = torch.abs(probs - 0.5) * 2.0\n",
    "\n",
    "    # wrong predictions: 1 if wrong, 0 if correct\n",
    "    hard_preds = (probs >= 0.5).float()\n",
    "    wrong = torch.abs(hard_preds - y_train)\n",
    "\n",
    "    # extra penalty:\n",
    "    #   - all wrong predictions get weighted by confidence\n",
    "    #   - WRONG & very high-confidence get an extra (confidence^2) penalty\n",
    "    high_conf_mask = confidence > 0.8\n",
    "    if high_conf_mask.any():\n",
    "        high_conf_wrong = (confidence[high_conf_mask] ** 2 * wrong[high_conf_mask]).mean()\n",
    "    else:\n",
    "        high_conf_wrong = torch.tensor(0.0, device=probs.device)\n",
    "\n",
    "    conf_wrong_penalty = (confidence * wrong).mean() + high_conf_wrong\n",
    "\n",
    "    loss = bce + lambda_penalty * conf_wrong_penalty\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Weights:\", model[0].weight.data)\n",
    "print(\"Bias:\", model[0].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f74981d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model decisions: 11 correct, 9 wrong out of 20\n",
      "\n",
      "By confidence level:\n",
      "High confidence: no examples\n",
      "Moderate confidence: 3 correct, 1 wrong out of 4 (acc= 0.75)\n",
      "Low confidence: 8 correct, 8 wrong out of 16 (acc= 0.50)\n",
      "\n",
      "Calibration per verbal label:\n",
      "Moderate confidence below average   n= 4, accuracy= 0.75\n",
      "Uncertain, could be average         n= 6, accuracy= 0.50\n",
      "Uncertain, could be below average   n=10, accuracy= 0.50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# y_test is 0/1, y_pred is probabilities, actual_above_below already built\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "\n",
    "def interpret_prediction(prob):\n",
    "    if prob >= 0.80:\n",
    "        return \"High confidence above average\"\n",
    "    elif prob >= 0.65:\n",
    "        return \"Moderate confidence above average\"\n",
    "    elif prob >= 0.55:\n",
    "        return \"Uncertain, could be above average\"\n",
    "    elif prob >= 0.45:\n",
    "        return \"Uncertain, could be average\"\n",
    "    elif prob >= 0.35:\n",
    "        return \"Uncertain, could be below average\"\n",
    "    elif prob >= 0.25:\n",
    "        return \"Moderate confidence below average\"\n",
    "    elif prob <= 0.20:\n",
    "        return \"High confidence below average\"\n",
    "    else:\n",
    "        return \"High confidence below average\"\n",
    "\n",
    "labels = []\n",
    "correct = []\n",
    "\n",
    "for i, prob in enumerate(y_pred):\n",
    "    p = prob.item()\n",
    "    lbl = interpret_prediction(p)\n",
    "    labels.append(lbl)\n",
    "\n",
    "    # model's hard prediction: above if p >= 0.5\n",
    "    pred_cls = 1.0 if p >= 0.5 else 0.0\n",
    "    true_cls = y_test[i].item()\n",
    "    correct.append(1 if pred_cls == true_cls else 0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "correct = np.array(correct)\n",
    "\n",
    "# counters for right and wrong decisions (overall)\n",
    "total_predictions = len(correct)\n",
    "total_correct = int(correct.sum())\n",
    "total_wrong = int(total_predictions - total_correct)\n",
    "print(f\"\\nModel decisions: {total_correct} correct, {total_wrong} wrong out of {total_predictions}\")\n",
    "\n",
    "# split counters by confidence level (high, moderate, low)\n",
    "high_total = high_correct = 0\n",
    "moderate_total = moderate_correct = 0\n",
    "low_total = low_correct = 0\n",
    "\n",
    "for lbl, is_corr in zip(labels, correct):\n",
    "    if lbl.startswith(\"High confidence\"):\n",
    "        high_total += 1\n",
    "        if is_corr:\n",
    "            high_correct += 1\n",
    "    elif lbl.startswith(\"Moderate confidence\"):\n",
    "        moderate_total += 1\n",
    "        if is_corr:\n",
    "            moderate_correct += 1\n",
    "    else:  # treat everything else as low confidence / uncertain\n",
    "        low_total += 1\n",
    "        if is_corr:\n",
    "            low_correct += 1\n",
    "\n",
    "def _print_conf_summary(name, total, correct):\n",
    "    if total == 0:\n",
    "        print(f\"{name}: no examples\")\n",
    "    else:\n",
    "        wrong = total - correct\n",
    "        acc = correct / total\n",
    "        print(f\"{name}: {correct} correct, {wrong} wrong out of {total} (acc={acc:5.2f})\")\n",
    "\n",
    "print(\"\\nBy confidence level:\")\n",
    "_print_conf_summary(\"High confidence\", high_total, high_correct)\n",
    "_print_conf_summary(\"Moderate confidence\", moderate_total, moderate_correct)\n",
    "_print_conf_summary(\"Low confidence\", low_total, low_correct)\n",
    "\n",
    "print(\"\\nCalibration per verbal label:\")\n",
    "for lbl in sorted(set(labels)):\n",
    "    mask = labels == lbl\n",
    "    n = mask.sum()\n",
    "    acc = correct[mask].mean() if n > 0 else float(\"nan\")\n",
    "    print(f\"{lbl:35s} n={n:2d}, accuracy={acc:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "af94536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Below Average']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48507434129714966"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hardcoded to test today's game\n",
    "X_test_np_today = [[36.6, 23, 7, 4, .522, .143, .688, .543, 1.000, 4, 5, 5, 16.5, 2, 29, 1, 0, 114.0, 119.7, -5.7, 96.9]]\n",
    "X_test_today = torch.tensor(X_test_np_today, dtype=torch.float32)\n",
    "y_today = model(X_test_today)\n",
    "\n",
    "# above average vs. below average calculation\n",
    "predicted_above_below = []\n",
    "actual_above_below = []\n",
    "for pred in y_today:\n",
    "    if pred.item() > 0.5:\n",
    "        predicted_above_below.append(\"Above Average\")\n",
    "    else:\n",
    "        predicted_above_below.append(\"Below Average\")\n",
    "\n",
    "for actual in y_test:\n",
    "    if actual.item() > 0.5:\n",
    "        actual_above_below.append(\"Above Average\")\n",
    "    else:\n",
    "        actual_above_below.append(\"Below Average\")\n",
    "\n",
    "print(predicted_above_below)\n",
    "y_today.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "31179068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28951922059059143 - Moderate confidence below average - Above Average\n",
      "0.33777040243148804 - Moderate confidence below average - Below Average\n",
      "0.48631641268730164 - Uncertain, could be average - Below Average\n",
      "0.46084344387054443 - Uncertain, could be average - Below Average\n",
      "0.3795250356197357 - Uncertain, could be below average - Below Average\n",
      "0.3901735544204712 - Uncertain, could be below average - Below Average\n",
      "0.44885846972465515 - Uncertain, could be below average - Below Average\n",
      "0.346571683883667 - Moderate confidence below average - Below Average\n",
      "0.43041670322418213 - Uncertain, could be below average - Above Average\n",
      "0.32709282636642456 - Moderate confidence below average - Below Average\n",
      "0.36155256628990173 - Uncertain, could be below average - Below Average\n",
      "0.3777649700641632 - Uncertain, could be below average - Above Average\n",
      "0.4702097773551941 - Uncertain, could be average - Above Average\n",
      "0.37064847350120544 - Uncertain, could be below average - Below Average\n",
      "0.44938892126083374 - Uncertain, could be below average - Above Average\n",
      "0.4544646143913269 - Uncertain, could be average - Above Average\n",
      "0.4364831745624542 - Uncertain, could be below average - Above Average\n",
      "0.47697532176971436 - Uncertain, could be average - Above Average\n",
      "0.3758336901664734 - Uncertain, could be below average - Above Average\n",
      "0.4861934185028076 - Uncertain, could be average - Below Average\n"
     ]
    }
   ],
   "source": [
    "interpreted_data = []\n",
    "for pred in y_pred:\n",
    "    interpreted_data.append(interpret_prediction(pred.item()))\n",
    "    print(str(pred.item()) + \" - \"  + interpret_prediction(pred.item()) + \" - \" + str(actual_above_below[len(interpreted_data)-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
