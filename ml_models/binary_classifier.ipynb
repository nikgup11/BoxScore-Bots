{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded520b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# if you ever use GPU, these help determinism\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c483359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts 'MM:SS' strings to float minutes\n",
    "def convert_mp(mp):\n",
    "    if isinstance(mp, str):\n",
    "        if ':' in mp:\n",
    "            try:\n",
    "                mins, secs = mp.split(':')\n",
    "                return int(mins) + int(secs) / 60\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                return float(mp)\n",
    "            except:\n",
    "                return None\n",
    "    elif isinstance(mp, (int, float)):\n",
    "        return float(mp)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c02b1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "#data = pd.read_csv('../data-collection/bbref_players_games_simple/g/gilgesh01_Shai_Gilgeous-Alexander_last3.csv')\n",
    "data = pd.read_csv('../data-collection/bbref_players_games_simple/gilgesh01_Shai_Gilgeous-Alexander_last3_with_opp_stats.csv')\n",
    "# make sure games are ordered oldest -> newest\n",
    "if 'Date' in data.columns:\n",
    "    data = data.sort_values('Date')\n",
    "\n",
    "# is_home: 1 if home, 0 if away\n",
    "data['is_home'] = (data['Unnamed: 5'] != '@').astype(float)\n",
    "# result_win: 1 if team won, 0 if lost\n",
    "data['result_win'] = data['Result'].str.startswith('W').astype(float)\n",
    "\n",
    "# features (previous game)\n",
    "features = [\n",
    "    'MP', 'FGA', '3PA', 'FTA',\n",
    "    'FG%', '3P%', '2P%', 'eFG%', 'FT%',\n",
    "    'TRB', 'AST', 'TOV',\n",
    "    'GmSc', '+/-', 'PTS', 'is_home', 'result_win',\n",
    "    'OppOffRtg', 'OppDefRtg', 'OppNetRtg', 'OppPace'\n",
    "]\n",
    "\n",
    "# convert MP to float minutes\n",
    "data['MP'] = data['MP'].apply(convert_mp)\n",
    "\n",
    "# convert numeric columns\n",
    "for col in features + ['PTS']:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# create TARGET from *current* game points (before shifting)\n",
    "current_ppg = 32.4\n",
    "data['above_ppg'] = (data['PTS'] > current_ppg).astype(float)\n",
    "target = ['above_ppg']\n",
    "\n",
    "# regression target: current game points\n",
    "data['pts_target'] = data['PTS']\n",
    "\n",
    "# now shift features so they refer to the *previous* game\n",
    "data[features] = data[features].shift(1)\n",
    "\n",
    "# drop first row and rows with NaNs\n",
    "data = data.dropna(subset=features + target + ['pts_target'])\n",
    "\n",
    "# numpy arrays\n",
    "X_np = data[features].values\n",
    "y_np = data[target].values\n",
    "y_pts_np = data[['pts_target']].values\n",
    "\n",
    "# chronological split: last 30 games as test\n",
    "# When holdout_for_eval=True, the last n_test games are kept ONLY for evaluation.\n",
    "# When holdout_for_eval=False, all games (including those last n_test) are used for training.\n",
    "holdout_for_eval = False  # set to False when training on all history to predict the next game\n",
    "n_test = 30\n",
    "if holdout_for_eval and n_test > 0:\n",
    "    X_train_np, X_test_np = X_np[:-n_test], X_np[-n_test:]\n",
    "    y_train_np, y_test_np = y_np[:-n_test], y_np[-n_test:]\n",
    "    y_pts_train_np, y_pts_test_np = y_pts_np[:-n_test], y_pts_np[-n_test:]\n",
    "else:\n",
    "    # train on all available games\n",
    "    X_train_np, y_train_np = X_np, y_np\n",
    "    y_pts_train_np = y_pts_np\n",
    "    # still define a test slice (e.g., last n_test games) for inspection if desired\n",
    "    if n_test > 0:\n",
    "        X_test_np, y_test_np = X_np[-n_test:], y_np[-n_test:]\n",
    "        y_pts_test_np = y_pts_np[-n_test:]\n",
    "    else:\n",
    "        # fall back to using the last game as a \"test\" point\n",
    "        X_test_np, y_test_np = X_np[[-1]], y_np[[-1]]\n",
    "        y_pts_test_np = y_pts_np[[-1]]\n",
    "\n",
    "print(len(X_train_np))\n",
    "print(len(X_test_np))\n",
    "\n",
    "# to torch\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float32)\n",
    "y_pts_train = torch.tensor(y_pts_train_np, dtype=torch.float32)\n",
    "y_pts_test = torch.tensor(y_pts_test_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeeb3e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[ 0.0099, -0.0755, -0.0299, -0.0260,  0.1496, -0.3189, -0.4328,  0.4441,\n",
      "          0.2995, -0.0502, -0.0104,  0.0623, -0.0100, -0.0048,  0.0471, -0.1188,\n",
      "          0.1738,  0.0738, -0.0535, -0.0439, -0.0216]])\n",
      "Bias: tensor([-0.0690])\n"
     ]
    }
   ],
   "source": [
    "# define model, loss function, and optimizer\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# per-sample BCE so we can apply recency weights\n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "epochs = 1000\n",
    "lambda_penalty = .8  # tune this (start small like 0.1â€“0.5)\n",
    "\n",
    "# recency weights: older games get lower weight, recent games higher\n",
    "n_train = X_train.shape[0]\n",
    "idx = torch.arange(n_train, dtype=torch.float32)\n",
    "# weights in [0.5, 1.0]; adjust endpoints as desired\n",
    "sample_weights = 0.5 + 0.5 * (idx / (n_train - 1))\n",
    "sample_weights = sample_weights.view(-1, 1)  # (n_train, 1) to broadcast with probs\n",
    "\n",
    "for _ in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    probs = model(X_train)\n",
    "\n",
    "    # base BCE loss (per-sample), then weighted by recency\n",
    "    bce_per_sample = criterion(probs, y_train)\n",
    "    bce = (bce_per_sample * sample_weights).mean()\n",
    "\n",
    "    # confidence in [0,1]: 0 at 0.5, 1 near 0 or 1\n",
    "    confidence = torch.abs(probs - 0.5) * 2.0\n",
    "\n",
    "    # wrong predictions: 1 if wrong, 0 if correct\n",
    "    hard_preds = (probs >= 0.5).float()\n",
    "    wrong = torch.abs(hard_preds - y_train)\n",
    "\n",
    "    # extra penalty:\n",
    "    #   - all wrong predictions get weighted by confidence AND recency\n",
    "    #   - WRONG & very high-confidence get an extra (confidence^2) penalty, also recency-weighted\n",
    "    high_conf_mask = confidence > 0.8\n",
    "    if high_conf_mask.any():\n",
    "        high_conf_wrong = (confidence[high_conf_mask] ** 2 *\n",
    "                           wrong[high_conf_mask] *\n",
    "                           sample_weights[high_conf_mask]).mean()\n",
    "    else:\n",
    "        high_conf_wrong = torch.tensor(0.0, device=probs.device)\n",
    "\n",
    "    conf_wrong_penalty = (confidence * wrong * sample_weights).mean() + high_conf_wrong\n",
    "\n",
    "    loss = bce + lambda_penalty * conf_wrong_penalty\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Weights:\", model[0].weight.data)\n",
    "print(\"Bias:\", model[0].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17f9d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression weights: tensor([[ 0.0984, -0.1195,  0.0830,  0.0467,  0.3072,  0.0984,  0.1540,  0.2231,\n",
      "          0.3378, -0.6510, -0.2725,  0.6931, -0.0432,  0.0019,  0.1442,  0.5517,\n",
      "          0.8516, -0.0021,  0.1245,  0.0629,  0.1424]])\n",
      "Regression bias: tensor([0.2702])\n"
     ]
    }
   ],
   "source": [
    "# regression model to predict points directly\n",
    "reg_model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 1)\n",
    ")\n",
    "reg_criterion = nn.MSELoss(reduction='none')\n",
    "reg_optimizer = torch.optim.Adam(reg_model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "reg_epochs = 1000\n",
    "\n",
    "for _ in range(reg_epochs):\n",
    "    reg_optimizer.zero_grad()\n",
    "    pts_pred = reg_model(X_train)\n",
    "    mse_per_sample = reg_criterion(pts_pred, y_pts_train)\n",
    "    # reuse same recency weights as classification\n",
    "    reg_loss = (mse_per_sample * sample_weights).mean()\n",
    "    reg_loss.backward()\n",
    "    reg_optimizer.step()\n",
    "\n",
    "print(\"Regression weights:\", reg_model[0].weight.data)\n",
    "print(\"Regression bias:\", reg_model[0].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f74981d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model decisions: 17 correct, 13 wrong out of 30\n",
      "\n",
      "By confidence level:\n",
      "High confidence: no examples\n",
      "Moderate confidence: 12 correct, 2 wrong out of 14 (acc= 0.86)\n",
      "Low confidence: 5 correct, 11 wrong out of 16 (acc= 0.31)\n",
      "\n",
      "Calibration per verbal label:\n",
      "Moderate confidence above average   n= 1, accuracy= 1.00\n",
      "Moderate confidence below average   n=13, accuracy= 0.85\n",
      "Uncertain, could be average         n=16, accuracy= 0.31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# y_test is 0/1, y_pred is probabilities, actual_above_below already built\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "\n",
    "def interpret_prediction(prob):\n",
    "    if prob >= 0.80:\n",
    "        return \"High confidence above average\"\n",
    "    elif prob >= 0.60:\n",
    "        return \"Moderate confidence above average\"\n",
    "    elif prob > 0.40:\n",
    "        return \"Uncertain, could be average\"\n",
    "    elif prob <= 0.4:\n",
    "        return \"Moderate confidence below average\"\n",
    "    elif prob <= 0.20:\n",
    "        return \"High confidence below average\"\n",
    "    else:\n",
    "        return \"High confidence below average\"\n",
    "\n",
    "labels = []\n",
    "correct = []\n",
    "\n",
    "for i, prob in enumerate(y_pred):\n",
    "    p = prob.item()\n",
    "    lbl = interpret_prediction(p)\n",
    "    labels.append(lbl)\n",
    "\n",
    "    # model's hard prediction: above if p >= 0.5\n",
    "    pred_cls = 1.0 if p >= 0.5 else 0.0\n",
    "    true_cls = y_test[i].item()\n",
    "    correct.append(1 if pred_cls == true_cls else 0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "correct = np.array(correct)\n",
    "\n",
    "# counters for right and wrong decisions (overall)\n",
    "total_predictions = len(correct)\n",
    "total_correct = int(correct.sum())\n",
    "total_wrong = int(total_predictions - total_correct)\n",
    "print(f\"\\nModel decisions: {total_correct} correct, {total_wrong} wrong out of {total_predictions}\")\n",
    "\n",
    "# split counters by confidence level (high, moderate, low)\n",
    "high_total = high_correct = 0\n",
    "moderate_total = moderate_correct = 0\n",
    "low_total = low_correct = 0\n",
    "\n",
    "for lbl, is_corr in zip(labels, correct):\n",
    "    if lbl.startswith(\"High confidence\"):\n",
    "        high_total += 1\n",
    "        if is_corr:\n",
    "            high_correct += 1\n",
    "    elif lbl.startswith(\"Moderate confidence\"):\n",
    "        moderate_total += 1\n",
    "        if is_corr:\n",
    "            moderate_correct += 1\n",
    "    else:  # treat everything else as low confidence / uncertain\n",
    "        low_total += 1\n",
    "        if is_corr:\n",
    "            low_correct += 1\n",
    "\n",
    "def _print_conf_summary(name, total, correct):\n",
    "    if total == 0:\n",
    "        print(f\"{name}: no examples\")\n",
    "    else:\n",
    "        wrong = total - correct\n",
    "        acc = correct / total\n",
    "        print(f\"{name}: {correct} correct, {wrong} wrong out of {total} (acc={acc:5.2f})\")\n",
    "\n",
    "print(\"\\nBy confidence level:\")\n",
    "_print_conf_summary(\"High confidence\", high_total, high_correct)\n",
    "_print_conf_summary(\"Moderate confidence\", moderate_total, moderate_correct)\n",
    "_print_conf_summary(\"Low confidence\", low_total, low_correct)\n",
    "\n",
    "print(\"\\nCalibration per verbal label:\")\n",
    "for lbl in sorted(set(labels)):\n",
    "    mask = labels == lbl\n",
    "    n = mask.sum()\n",
    "    acc = correct[mask].mean() if n > 0 else float(\"nan\")\n",
    "    print(f\"{lbl:35s} n={n:2d}, accuracy={acc:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af94536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertain, could be average\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4526937007904053"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hardcoded to test today's game\n",
    "X_test_np_today = [[36.6, 23, 7, 4, .522, .143, .688, .543, 1.000, 4, 5, 5, 16.5, 2, 29, 1, 0, 114.0, 119.7, -5.7, 96.9]]\n",
    "X_test_today = torch.tensor(X_test_np_today, dtype=torch.float32)\n",
    "y_today = model(X_test_today)\n",
    "\n",
    "# above average vs. below average calculation\n",
    "predicted_above_below = []\n",
    "actual_above_below = []\n",
    "for pred in y_today:\n",
    "    if pred.item() > 0.5:\n",
    "        predicted_above_below.append(\"Above Average\")\n",
    "    else:\n",
    "        predicted_above_below.append(\"Below Average\")\n",
    "\n",
    "for actual in y_test:\n",
    "    if actual.item() > 0.5:\n",
    "        actual_above_below.append(\"Above Average\")\n",
    "    else:\n",
    "        actual_above_below.append(\"Below Average\")\n",
    "\n",
    "print(interpret_prediction(y_today.item()))\n",
    "y_today.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31179068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6068565845489502 - Moderate confidence above average - Above Average\n",
      "0.5151800513267517 - Uncertain, could be average - Below Average\n",
      "0.3709820508956909 - Moderate confidence below average - Below Average\n",
      "0.44218218326568604 - Uncertain, could be average - Above Average\n",
      "0.422963410615921 - Uncertain, could be average - Above Average\n",
      "0.30150699615478516 - Moderate confidence below average - Below Average\n",
      "0.49167194962501526 - Uncertain, could be average - Above Average\n",
      "0.2954097092151642 - Moderate confidence below average - Below Average\n",
      "0.3839735984802246 - Moderate confidence below average - Below Average\n",
      "0.3854684829711914 - Moderate confidence below average - Above Average\n",
      "0.4051227569580078 - Uncertain, could be average - Above Average\n",
      "0.32889705896377563 - Moderate confidence below average - Below Average\n",
      "0.5125371813774109 - Uncertain, could be average - Below Average\n",
      "0.43428835272789 - Uncertain, could be average - Below Average\n",
      "0.3070039451122284 - Moderate confidence below average - Below Average\n",
      "0.36014920473098755 - Moderate confidence below average - Below Average\n",
      "0.4871944785118103 - Uncertain, could be average - Below Average\n",
      "0.26048827171325684 - Moderate confidence below average - Below Average\n",
      "0.47564464807510376 - Uncertain, could be average - Above Average\n",
      "0.34405073523521423 - Moderate confidence below average - Below Average\n",
      "0.3511461019515991 - Moderate confidence below average - Below Average\n",
      "0.39274922013282776 - Moderate confidence below average - Above Average\n",
      "0.5480105876922607 - Uncertain, could be average - Above Average\n",
      "0.34111782908439636 - Moderate confidence below average - Below Average\n",
      "0.5136427879333496 - Uncertain, could be average - Above Average\n",
      "0.4535212218761444 - Uncertain, could be average - Above Average\n",
      "0.4919491112232208 - Uncertain, could be average - Above Average\n",
      "0.5399399399757385 - Uncertain, could be average - Above Average\n",
      "0.4626881778240204 - Uncertain, could be average - Above Average\n",
      "0.5062880516052246 - Uncertain, could be average - Below Average\n"
     ]
    }
   ],
   "source": [
    "interpreted_data = []\n",
    "for pred in y_pred:\n",
    "    interpreted_data.append(interpret_prediction(pred.item()))\n",
    "    print(str(pred.item()) + \" - \"  + interpret_prediction(pred.item()) + \" - \" + str(actual_above_below[len(interpreted_data)-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
